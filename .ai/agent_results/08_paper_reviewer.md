# Simulated Peer Review: C&V Validation Module for LLM-ABM

**Reviewer**: Dr. Elena Volkov (Associate Editor, Water Resources Research)
**Expertise**: ABM validation, socio-hydrological modeling, pattern-oriented modeling
**Date**: 2026-02-14
**Manuscript**: "Construct & Validation Framework for LLM-Driven Agent-Based Models"

---

## Summary

The authors present a three-level validation framework (L1 Micro, L2 Macro, L3 Cognitive) for LLM-driven Agent-Based Models, applied to a 400-agent flood adaptation simulation in the Passaic River Basin, NJ. The framework evaluates behavioral fidelity against Protection Motivation Theory (PMT) at the individual level (CACR, hallucination rate, behavioral entropy) and against eight empirical benchmarks at the aggregate level (EPI). The L3 cognitive level uses ICC and eta-squared to assess persona discriminability prior to the main experiment.

The framework is well-structured and represents a genuine attempt to bring rigor to LLM-ABM validation -- a field sorely lacking it. The code is modular, documented, and extensible via a `BehavioralTheory` protocol and `BenchmarkRegistry`. However, I have five major methodological concerns that must be addressed before this work can claim scientific defensibility.

---

## Major Concerns

### MC1: CACR Circularity -- Self-Consistency Masquerading as Construct Validity

**Severity: Critical**

The Construct-Action Coherence Rate (CACR) checks whether LLM-generated TP/CP labels are consistent with LLM-generated actions. The authors acknowledge this in their README ("CACR checks whether LLM-generated TP/CP labels are consistent with actions = self-consistency, not construct validity"), which I credit, but the framing throughout the rest of the module still treats high CACR as evidence of behavioral validity.

The fundamental problem: an LLM that always outputs `{"TP_LABEL": "VH", "CP_LABEL": "VH"}` and `"action": "buy_insurance"` would achieve CACR = 1.0 while telling us nothing about whether the agent actually perceives high threat. The construct labels are ungrounded -- they are not measured by an independent instrument (e.g., a validated PMT survey scale), they are generated by the same model that selects the action.

The authors' defense via CACR decomposition (separating `cacr_raw` from `cacr_final`) is interesting but insufficient. Even high `cacr_raw` only proves the LLM has learned the PMT lookup table -- not that it reasons through threat and coping appraisal.

**What would address this**: (a) External construct validation -- present the same scenario to the LLM but elicit TP/CP through a validated survey instrument (Grothmann & Reusswig 2006, items) rather than free-form labels, then compare; (b) Construct perturbation tests -- systematically vary one construct input (e.g., flood depth) and verify that TP labels shift monotonically; (c) At minimum, report the conditional entropy H(Action | TP, CP) to show that constructs carry information beyond a marginal action distribution.

The authors' L3 cognitive validation (ICC, eta-squared, directional sensitivity) partially addresses this for archetypes, but L3 is run on 15 handpicked extreme personas, not on the 400 survey-based agents used in the main experiment. The gap between L3 and L1 populations is not bridged.

### MC2: Benchmark Range Width and Random-Model Pass Probability

**Severity: High**

The eight empirical benchmarks have wide ranges. Let me compute the approximate acceptance probability under a random baseline:

| Benchmark | Range | Width | Plausible domain | P(random in range) |
|-----------|-------|-------|-------------------|--------------------|
| insurance_rate_sfha | 0.30-0.60 | 0.30 | 0-1 | ~0.30 |
| insurance_rate_all | 0.15-0.55 | 0.40 | 0-1 | ~0.40 |
| elevation_rate | 0.10-0.35 | 0.25 | 0-1 | ~0.25 |
| buyout_rate | 0.05-0.25 | 0.20 | 0-1 | ~0.20 |
| do_nothing_postflood | 0.35-0.65 | 0.30 | 0-1 | ~0.30 |
| mg_adaptation_gap | 0.05-0.30 | 0.25 | 0-0.5 | ~0.50 |
| renter_uninsured | 0.15-0.40 | 0.25 | 0-1 | ~0.25 |
| insurance_lapse | 0.15-0.30 | 0.15 | 0-1 | ~0.15 |

Under a simplistic uniform-random assumption, the probability of passing any single benchmark is 15-50%. With the weighted EPI threshold of 0.60, a random model that passes 5-6 of 8 benchmarks (weighted) could clear the bar. A back-of-envelope calculation: if we assume benchmark outcomes are independent and each passes with ~25% probability, then P(EPI >= 0.60) is non-trivial (order of ~5-10% even under independence, higher if actions cluster in realistic ranges).

**What would address this**: (a) Report the actual pass probability of a null model -- run the EPI computation on N=1000 random action sequences (uniform over the action space) and report the empirical null distribution of EPI. If a random model achieves EPI >= 0.60 more than 5% of the time, the threshold is too lenient. (b) Consider distance-based scoring (how close to range center) rather than binary in/out-of-range. (c) Tighten ranges where literature supports narrower bounds -- the insurance lapse range (0.15-0.30) is the tightest and most defensible; others should follow suit.

I note that the YAML config and the Python code define **different** ranges for some benchmarks (the `CLAUDE.local.md` shows narrower B1: 0.30-0.50 and B3: 0.03-0.12, while the actual code uses 0.30-0.60 and 0.10-0.35). Which are the authoritative values? This discrepancy undermines reproducibility.

### MC3: Calibration vs. Validation Separation

**Severity: High**

The README states as a key design decision: "Explicitly label which benchmarks were iterated during development (calibration targets) vs. held out (validation targets)." This is stated as an aspiration but is **not implemented anywhere in the code**. No benchmark is tagged as "calibration" or "holdout" in the YAML, in the Python definitions, or in the output reports.

From the project memory, it is clear that the benchmarks were iteratively tuned during development (the L2 Calibration History shows EPI progressing from 0.22 to 0.67 to 0.87 through successive prompt and configuration iterations). This means **all 8 benchmarks were effectively calibration targets**. The EPI score is therefore a measure of calibration fit, not validation performance.

This is the most consequential issue for publication. Without at least a 4:4 split (4 calibration, 4 held-out) or, better, k-fold cross-validation across benchmark subsets, the L2 results are unfalsifiable -- the authors tuned the model until benchmarks passed, then reported those same benchmarks as validation.

**What would address this**: (a) Explicitly partition benchmarks into calibration (used during prompt engineering) and validation (never seen during development) sets, and report EPI separately for each; (b) Use temporal holdout -- calibrate on years 1-6, validate on years 7-13; (c) At minimum, add a column `role: calibration | validation` to `flood_benchmarks.yaml` and report both EPIs.

### MC4: "Structural Plausibility" as an Unfalsifiable Claim

**Severity: Moderate-High**

The framework explicitly disclaims predictive accuracy and instead targets "structural plausibility" -- that agent behaviors are consistent with behavioral theory and aggregate patterns match empirical literature. This is a reasonable epistemological position for ABMs (see Grimm et al. 2005, Sun et al. 2016), but the authors do not sufficiently grapple with what would constitute **falsification** under their own criteria.

Currently, the framework can only **fail** if:
- CACR < 0.75 (but see MC1 -- this is easy to game)
- R_H > 0.10 (but only 3 hallucination rules are checked)
- EPI < 0.60 (but see MC2 -- ranges are wide)

The hallucination detection (`hallucinations/flood.py`) checks only three conditions: re-elevation, decisions after buyout, and renters elevating. This is an extremely sparse set. There are many other physically impossible or highly implausible actions not checked: e.g., buying insurance when already insured in the same year, elevating a property with insufficient funds, relocating to a higher-risk zone. The low R_H threshold (10%) combined with minimal hallucination rules means virtually any model will pass.

**What would address this**: (a) Enumerate a comprehensive set of domain impossibilities, ideally derived from expert elicitation; (b) Report not just R_H but a **severity-weighted** hallucination score; (c) Define what specific empirical patterns the model **should not** produce (anti-patterns) and test for their absence -- e.g., adaptation should not decrease after flooding, insurance rates should not be uniform across flood zones.

### MC5: Missing Statistical Inference

**Severity: Moderate**

All metrics are reported as point estimates with no uncertainty quantification:

- **CACR**: No confidence interval. With 5,200 decisions (400 agents x 13 years), even small deviations from 0.75 are statistically meaningful, but the threshold comparison is done against a single point estimate without a binomial CI.
- **EPI**: No bootstrap confidence interval across seeds. The pilot ran with a single seed (seed 42). With N=1 experiment, we have no sense of variability.
- **Benchmarks**: No standard errors on benchmark values. The `_compute_benchmark` functions return single floats.
- **No power analysis**: How many agents/years are needed to distinguish the model's EPI from a random baseline at alpha=0.05?

The code computes CACR as `coherent / cacr_eligible` but never constructs a Wilson or Clopper-Pearson interval. For 400 agents over 13 years, the standard error on a proportion near 0.75 is approximately sqrt(0.75 * 0.25 / 5200) = 0.006, meaning the 95% CI would be roughly +/- 0.012. This is small, but the authors should still report it.

More critically, the EPI from a single seed tells us nothing about reproducibility. The planned 400x13yr production run should be repeated with multiple seeds, and EPI variability across seeds should be reported.

**What would address this**: (a) Bootstrap CIs for all metrics (resample traces with replacement, compute metric, report 2.5th-97.5th percentiles); (b) Multi-seed experiments with EPI mean and standard deviation; (c) A null-model comparison (random agent EPI distribution) as a statistical baseline.

---

## Minor Concerns

### MN1: Inconsistent Benchmark Values Across Documentation

The `CLAUDE.local.md` quick reference lists different benchmark ranges than the actual code (`flood.py` and `flood_benchmarks.yaml`). For example, B1 is listed as 0.30-0.50 in the documentation but 0.30-0.60 in the code; B3 is 0.03-0.12 vs. 0.10-0.35. The YAML and Python definitions agree, but the project documentation is stale. This is a reproducibility concern.

### MN2: Hard-coded Agent ID Threshold for Owner/Renter Detection

In `l1_micro.py:181`, the CACR decomposition infers agent type from agent ID numbering: `if num > 200: return PMT_RENTER_RULES`. This is fragile and will break if agent IDs are renumbered. The agent type should come from the trace or profile data.

### MN3: EBE Threshold Justification

The EBE ratio threshold (0.1 < ratio < 0.9) is stated without justification. Why these specific bounds? What empirical or theoretical basis exists for concluding that a behavioral entropy ratio of 0.11 is acceptable but 0.09 is not? The POM framework does not prescribe entropy bounds.

### MN4: No Comparison to POM or TRACE

The README cites Grimm et al. (2005) as inspiration but does not systematically compare the C&V framework to the Pattern-Oriented Modeling (POM) protocol or the TRACE documentation framework (Schmolke et al. 2010). A table showing which POM criteria are addressed and which gaps remain would strengthen the methodological positioning.

### MN5: Missing Temporal Metrics

The README acknowledges the absence of temporal trajectory validation. For a 13-year simulation, this is a significant gap. Post-flood adaptation spikes, insurance survival curves, and adaptation S-curves are standard in the flood adaptation literature (Bubeck et al. 2012, Michel-Kerjan et al. 2012). Without temporal validation, the framework cannot distinguish a model that gets the right long-run average from one that captures the dynamics.

### MN6: Code Quality -- Generally Good

- Clean separation of concerns (theories, metrics, benchmarks, hallucinations, IO, reporting)
- The `BehavioralTheory` protocol is well-designed and genuinely extensible
- The `BenchmarkRegistry` decorator pattern is idiomatic
- Proper handling of UNKNOWN sentinels and extraction failures
- UTF-8 encoding consistently specified
- Dataclass usage is clean and appropriate

The only code-quality concern is the exception silencing in `BenchmarkRegistry.dispatch()` (catches all exceptions and returns None), which could mask bugs during development.

---

## Questions for Authors

1. Has CACR been validated against an external instrument? If you present the same scenario but elicit TP/CP via structured survey items (not free-form labels), does the LLM produce consistent construct levels?

2. What is the EPI of a random-action model? Please report this null distribution.

3. Which of the 8 benchmarks were held out during prompt engineering? If none, the entire L2 evaluation is circular.

4. Why are only 3 hallucination rules defined for a domain with dozens of physical constraints? How were these 3 selected?

5. How does EPI vary across random seeds? A single seed is insufficient for any scientific claim.

---

## Verdict

**Recommendation: Major Revision**

**Justification**: The C&V framework represents a meaningful contribution to the nascent field of LLM-ABM validation. The three-level architecture is sensible, the code is well-organized, and the explicit acknowledgment of limitations (particularly CACR circularity) is commendable and unusual in this literature.

However, the five major concerns identified above -- particularly MC1 (CACR circularity without external grounding), MC2 (no random-baseline comparison), and MC3 (no calibration/validation separation) -- collectively undermine the scientific defensibility of the validation claims. As presented, the framework demonstrates internal consistency and calibration fit, but not validation in the epistemological sense required for publication in WRR.

The good news: all five major concerns are addressable without fundamental redesign. The authors need to:
1. Add a null-model EPI distribution (MC2) -- this is a weekend of compute
2. Partition benchmarks into calibration/validation sets (MC3) -- this requires honesty about which were tuned
3. Add bootstrap CIs and multi-seed reporting (MC5) -- straightforward engineering
4. Expand hallucination rules (MC4) -- domain expertise exercise
5. Plan external construct validation as future work, but at minimum add perturbation tests (MC1)

I would be happy to review a revised version. The framework has the potential to become a community standard for LLM-ABM validation if these methodological gaps are closed.

---

*Reviewer declaration: No conflicts of interest. I have not seen this manuscript before and have no collaboration with the authors.*
