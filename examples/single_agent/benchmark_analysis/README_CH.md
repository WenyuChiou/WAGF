# 記憶體基準分析報告

## 核心問題：為何套用治理框架後模型行為不同？

### 答案：三個根本原因

| 原因                      | 解釋                                    | 影響                                                  |
| ------------------------- | --------------------------------------- | ----------------------------------------------------- |
| **1. 驗證 ≠ 推理**        | 100% 通過表示格式正確，而非推理相同     | 模型對相同提示的解釋不同                              |
| **2. 記憶窗口 (top_k=3)** | 僅保留 3 條記憶；洪水歷史被社交觀察擠掉 | 敏感模型（Llama）在傳統版過度反應，但治理阻止這種行為 |
| **3. 治理執行**           | `strict` 配置阻止不合邏輯的組合         | 強制執行傳統版缺失的一致性                            |

### 關鍵洞察：治理可以減少搬遷

令人驚訝的發現：**Llama 3.2 在治理下搬遷更少**（95 → 79）。

**為什麼？** 傳統系統允許不合邏輯的行為：

- 代理可以說「低威脅」但仍然搬遷（恐慌驅動）
- 治理阻止這種情況：「低/中威脅 + 搬遷」被拒絕
- 這迫使以前恐慌的代理更加理性

---

## 比較圖表

![比較](old_vs_window_vs_importance_3x4.png)

_注：每年僅顯示活躍代理（已搬遷代理從後續年份中排除）_

---

## 模型特定分析

### Gemma 3 (4B)

| 指標       | 傳統版 | Window | Importance | 變化            |
| ---------- | ------ | ------ | ---------- | --------------- |
| 最終搬遷數 | 6      | 18     | 25         | **+12 (+200%)** |

**行為變化：**

- Gemma 在傳統版非常保守（僅 6 次搬遷）
- 治理增加了搬遷，因為 Gemma 現在更誠實地評估威脅
- 當 Gemma 說「高威脅」時，治理強制採取行動

**洪水年響應：**

| 年份      | 傳統版 | Window | Importance |
| --------- | ------ | ------ | ---------- |
| 3（洪水） | 0      | 2      | 2          |
| 4（洪水） | 0      | 0      | 2          |
| 9（洪水） | 2      | 3      | 2          |

---

### Llama 3.2 (3B)

| 指標       | 傳統版 | Window | Importance | 變化           |
| ---------- | ------ | ------ | ---------- | -------------- |
| 最終搬遷數 | 95     | 79     | 68         | **-16 (-17%)** |

**行為變化：**

- Llama 在傳統版非常反應靈敏（95 次搬遷，所有模型中最高）
- 治理減少了搬遷，因為傳統 Llama 有許多不合邏輯的決策
- 「低威脅 + 搬遷」組合在傳統版很常見，但現在被阻止

**根本原因分析：**

1. 傳統 Llama 對社交觀察敏感（「X% 鄰居已搬遷」）
2. 這觸發了即使沒有洪水記憶也會恐慌搬遷
3. 治理現在要求威脅評估與行動匹配
4. 結果：更理性，更少恐慌搬遷

**洪水年響應：**

| 年份      | 傳統版 | Window | Importance |
| --------- | ------ | ------ | ---------- |
| 3（洪水） | 21     | 7      | 9          |
| 4（洪水） | 18     | 11     | 8          |
| 9（洪水） | 11     | 1      | 1          |

**關鍵發現：** 傳統 Llama 對洪水響應最高，但治理後的 Llama 更加穩重。

---

### DeepSeek-R1 (8B)

| 指標       | 傳統版 | Window | Importance | 變化            |
| ---------- | ------ | ------ | ---------- | --------------- |
| 最終搬遷數 | 14     | 0      | N/A        | **-14 (-100%)** |

**行為變化：**

- DeepSeek 在傳統版適度保守（14 次搬遷）
- 治理消除了所有搬遷
- DeepSeek 很少將威脅評估為「高」，因此不會觸發強制行動

**根本原因分析：**

1. DeepSeek 即使發生洪水也輸出「低」或「中」威脅
2. 沒有「高威脅」，治理無法強制搬遷
3. 結果：超保守行為被保持

---

### GPT-OSS (20B)

| 指標       | 傳統版 | Window | Importance | 變化       |
| ---------- | ------ | ------ | ---------- | ---------- |
| 最終搬遷數 | 0      | N/A    | N/A        | **無變化** |

**注：** GPT-OSS 的 Window 和 Importance 運行尚未完成。

---

## 驗證摘要

| 模型         | 記憶類型   | 總數 | 重試 | 失敗 | 解析警告 |
| ------------ | ---------- | ---- | ---- | ---- | -------- |
| Gemma 3 (4B) | Window     | 1000 | 0    | 0    | 977      |
| Gemma 3 (4B) | Importance | 1000 | 0    | 0    | 978      |

**結論：** 所有模型的驗證通過率為 100%。解析警告僅供參考。

---

## 為何驗證通過 ≠ 相同行為

```
┌─────────────────────────────────────────────────────────────┐
│  驗證檢查：                                                  │
│  ✓ 輸出 JSON 是否有效？                                      │
│  ✓ 是否包含必要欄位（skill_name、TP、CP）？                  │
│  ✓ TP/CP 是否為 H/M/L 之一？                                 │
│  ✓ skill_name 是否為有效選項？                               │
│                                                              │
│  驗證不檢查：                                                │
│  ✗ 威脅評估是否正確？                                        │
│  ✗ 推理是否合邏輯？                                          │
│  ✗ 代理是否記得洪水？                                        │
└─────────────────────────────────────────────────────────────┘
```

每個 LLM 具有不同的：

- 對社交觀察的敏感度
- 對洪水事件的記憶
- 對「高」vs「中」威脅的解釋
- 風險容忍度

**治理強制一致性，而非正確性。**
