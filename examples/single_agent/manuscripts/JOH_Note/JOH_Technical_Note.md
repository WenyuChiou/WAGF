# JOH Technical Note (v5): The Cognitive Architecture of Adaptation

> **Version**: 5.0 (Cognitive Architecture Edition)
> **Date**: January 2026

**Title**: The Cognitive Equalizer: How Governance Architectures Stabilize Small Language Models in Hydro-Social Simulation

**Abstract**
As Agentic AI scales, a critical question emerges: Do we need larger models (32B+) for rational behavior, or better architectures? This Technical Note presents the **"Cognitive Equalizer Hypothesis"**. By subjecting the DeepSeek R1 family (1.5B–32B) to three distinct governance architectures, we identify a "Universal Stabilizer" effect. We classify these architectures as **The Child (Mode A)**, **The Constraint (Mode B)**, and **The Sage (Mode C)**. Our findings demonstrate that Mode C (Reflective Governance) allows small 1.5B models to exhibit the behavioral stability and reasoning depth typically reserved for 32B models, effectively solving the "Small Model Instability" problem through architectural scaffolding rather than parameter scaling.

**Keywords**: Socio-hydrology, Governance Scaling Laws, DeepSeek R1, Generative Agents, Cognitive Equalizer.

## 1. Introduction: The Cognitive Gap in Agent-Based Modeling

The integration of Large Language Models (LLMs) into Agent-Based Modeling (ABM) promises a "Generative Revolution" in socio-hydrology (Park et al., 2023; Xi et al., 2023). By replacing static laws with fluid neural networks, we aim to simulate complex adaptation strategies like migration and insurance with high fidelity. However, a critical "Validity Gap" remains (Ji et al., 2023). Unconstrained LLMs suffer from "Cognitive Instability"—oscillating between panic and apathy due to stochastic entropy. Our **Governed Broker Framework** addresses this by introducing a "System 2" cognitive prosthetic, ensuring that agent rationality is not just a function of model size, but of architectural governance. This leads to our first inquiry: **Can a Reflective Governance Framework enforce rational adaptation in unstable agents (SQ1)?**

Beyond basic rationality, we must address the "Small Model Problem". Current stable reasoning typically requires massive models (e.g., GPT-4), which are computationally prohibitive for large-scale simulations ($N=10^5$). We hypothesize that cognitive architecture can serve as a scalable "Equalizer", allowing small 1.5B models to mimic the stability of larger counterparts. To validate this, we conduct rigorous **Stress Testing** (Pressure Tests), subjecting agents to high-frequency noise to measure their robustness. This motivates our second question: **Does this framework effectively stabilize small-parameter models to match large-model performance (SQ2)?**

Finally, adaptation is inherently social. In hydro-social systems, decisions are driven not just by individual risk perception but by community feedback and transparency. However, social cues can also introduce "Contagion Effects" or "Groupthink", potentially destabilizing the governed agents. We therefore investigate the "Social Cost" of agency: **How does social transparency affect the stability of these governed agents (SQ3)?**

## 1.4 Methodology & Metrics

To rigorously evaluate these architectures, we employ a **"Cognitive Appraisal Profile"** to diagnose the internal reasoning process (System 2) rather than just the final output:

- **TP (HiTA)**: High Threat Appraisal rate (Paranoia).
- **CP (HiCA)**: High Coping Appraisal rate (Confidence).
- **Alignment (Align)**: Probability of Action given High Appraisal (Rationality).
- **Intv**: Governance Intervention Count (Cognitive Deficit).
- **Stability (FF)**: Flip-Flop Rate (Inter-annual decision consistency).

---

### 1.4 Methodology: The Three Archetypes

To investigate these problems, we map our experimental groups to three "Developmental Stages" of AI reliability:

**Type A: The Unstructured Child (Baseline)**

- **Cognitive State**: **Reactive**. Driven by immediate context window noise.
- **Metaphor**: An impulsive child who forgets the past and reacts only to the present.

**Type B: The Constrained Adult (Mode I)**

- **Cognitive State**: **Compliant**. Externalizes reasoning to "System Rules".
- **Metaphor**: A bureaucratic adult who follows the building code but lacks deep introspection. Efficient, resilient, but rigid.

**Type C: The Reflective Sage (Mode II, "The Ideal")**

- **Cognitive State**: **Agency**. Internalizes reasoning via Reflection and Memory.
- **Metaphor**: A wise elder who makes hard decisions (including Retreat) based on deep historical synthesis.

### 1.5 Conclusion: The Ultimate Goal

Ultimately, the purpose of this framework is not merely to build "smarter" agents, but to **operationalize Large Language Models as rigorous scientific instruments**. By solving the trilemma of Rationality, Scale, and Cost, we aim to provide a blueprint for the **reasonable and reliable use of LLMs** in high-stakes socio-hydrological simulations, ensuring that the stochastic nature of AI does not compromise the validity of scientific inquiry.

### SQ2: The Stability Question (Scaling)

> _Can the Cognitive Architecture stabilize the reasoning trajectories of small-parameter models (1.5B) to match the behavioral fidelity of large-parameter models?_

- **Metric**: **Validator Error Profile**.
  - Definition: The distribution of **Syntactic Errors** (JSON Malformation) vs. **Semantic Errors** (Constraint Violations / Model-Governance Mismatch).
  - _Purpose_: Proves the "Cognitive Equalizer" hypothesis by showing Reduced Entropy in Group C (1.5B) vs Group A (1.5B).

### SQ3: The Cost Question (Efficiency)

> _What is the computational and temporal cost of deploying Reflective Governance, and does the gain in stability justify the overhead?_

- **Metric**: **Runtime Duration & Token Volume**.
  - _Purpose_: Quantifies the "Price of Agency"—the extra compute required to turn a Small Model into a Wise Agent.

## 2. Preliminary Results Summary

_For detailed analysis of Rationality (SQ1), please refer to the standalone [SQ1 Analysis Report](../analysis/SQ1_Analysis_Report.md)._

Our pilot data indicates a "Cognitive Equalizer" effect where architecture compensates for model size deficits in small models (1.5B), but acts as a constraint in larger models (14B).

## 3. Governance as a Memory Prosthetic

We further identify that the difference between Type B and Type C is primarily **Memory Management**:

- **Type B (Resilience)**: Relies on **Forgetting**. By allowing trauma to decay (Window Memory), the agent's fear subsides, allowing the Governance Framework to act as a "Prosthetic Rationality" tailored for investment (Elevation).
- **Type C (Agency)**: Relies on **Remembrance**. By preserving trauma (Importance Memory), the agent's fear overrides standard rules, activating "Safety Valve" clauses that permit Rational Retreat.

## 4. Policy Implications

- **Use Type B**: When the goal is **Infrastructure Preservation** (Engineering Resilience).
- **Use Type C**: When the goal is **Social Realism** (Predicting Climate Migration).

---

**References**

- Di Baldassarre, G., et al. (2013). Socio-hydrology.
- Park, J. S., et al. (2023). Generative Agents.
